This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, line numbers have been added, content has been compressed (code blocks are separated by ⋮---- delimiter), security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Line numbers have been added to the beginning of each line
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    baseline-map.yml
    document_update.yml
    update-baseline-map.yml
agent/
  config/
    __init__.py
    llm_config.py
  database/
    migrations/
      001_create_baseline_map_tables.sql
      002_add_vector_embeddings.sql
    __init__.py
    baseline_map_repository.py
    supabase_client.py
    vector_search_repository.py
  llm/
    __init__.py
    embedding_client.py
    llm_client.py
  models/
    __init__.py
    docureco_models.py
  workflows/
    __init__.py
    baseline_map_creator.py
    baseline_map_updater.py
    document_update_recommendator.py
  __init__.py
  baseline_map_creator.py
  baseline_map_updater.py
  config.env.example
  main.py
  README.md
  requirements.txt
docs/
  SUPABASE_SETUP.md
.gitignore
env.example
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/baseline-map.yml">
name: 'Docureco Agent: Baseline Map'

on:
  workflow_dispatch:
    inputs:
      repository:
        description: 'Repository name (owner/repo)'
        required: true
        default: 'mhelmih/docureco'
      branch:
        description: 'Branch name to analyze'
        required: true
        default: 'main'
      force_recreate:
        description: 'Force recreate if baseline map exists'
        required: false
        default: false
        type: boolean

env:

  GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
  GROK_BASE_URL: ${{ secrets.GROK_BASE_URL }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}


  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}


  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  create-baseline-map:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        cd agent
        pip install -r requirements.txt

    - name: Validate inputs
      run: |
        echo "Repository: ${{ github.event.inputs.repository }}"
        echo "Branch: ${{ github.event.inputs.branch }}"
        echo "Force recreate: ${{ github.event.inputs.force_recreate }}"


        if [[ ! "${{ github.event.inputs.repository }}" =~ ^[a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+$ ]]; then
          echo "Error: Repository must be in format 'owner/repo'"
          exit 1
        fi

    - name: Create baseline traceability map
      run: |
        cd agent
        python -c "
        import asyncio
        import os
        import sys
        from workflows.baseline_map_creator import create_baseline_map_creator

        async def main():
            try:
                print('🚀 Starting baseline map creation...')

                # Create workflow instance
                creator = create_baseline_map_creator()

                # Execute baseline map creation
                repository = '${{ github.event.inputs.repository }}'
                branch = '${{ github.event.inputs.branch }}'
                force_recreate = '${{ github.event.inputs.force_recreate }}' == 'true'

                print(f'📊 Analyzing repository: {repository}:{branch}')

                # Override existing check if force recreate is enabled
                if force_recreate:
                    print('⚠️  Force recreate enabled - will overwrite existing baseline map')

                result = await creator.execute(repository, branch)

                # Print results
                print('\\n📈 Baseline Map Creation Results:')
                print(f'Repository: {result.repository}:{result.branch}')
                print(f'Requirements: {len(result.requirements)}')
                print(f'Design Elements: {len(result.design_elements)}')
                print(f'Code Components: {len(result.code_components)}')
                print(f'Traceability Links: {len(result.traceability_links)}')

                if result.errors:
                    print('\\n⚠️  Errors encountered:')
                    for error in result.errors:
                        print(f'  - {error}')

                print(f'\\n✅ Baseline map creation completed: {result.current_step}')

            except Exception as e:
                print(f'❌ Baseline map creation failed: {str(e)}')
                sys.exit(1)

        asyncio.run(main())
        "

    - name: Upload baseline map artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: baseline-map-${{ github.event.inputs.repository }}-${{ github.event.inputs.branch }}
        path: |
          agent/logs/
          agent/output/
        retention-days: 30

    - name: Create summary
      if: always()
      run: |
        echo "## 📊 Baseline Map Creation Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Repository:** \`${{ github.event.inputs.repository }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Branch:** \`${{ github.event.inputs.branch }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Force Recreate:** \`${{ github.event.inputs.force_recreate }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "
        echo "- Started by: @${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "- Workflow: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
        echo "- Run ID: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
</file>

<file path=".github/workflows/document_update.yml">
name: 'Docureco Agent: PR Analysis'

on:
  pull_request:
    types: [opened, synchronize, reopened]
  pull_request_target:
    types: [closed]
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number (for manual analysis)'
        required: false
        type: string

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:

  analyze:
    if: >
      github.event_name == 'pull_request' ||
      (github.event_name == 'workflow_dispatch' && inputs.action == 'analyze_pr')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('agent/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r agent/requirements.txt

      - name: Validate environment variables
        run: |
          echo "Checking required environment variables..."
          if [ -z "${{ secrets.GROK_API_KEY }}" ]; then
            echo "Warning: GROK_API_KEY not set, will try OpenAI fallback"
          fi
          if [ -z "${{ secrets.OPENAI_API_KEY }}" ]; then
            echo "Warning: OPENAI_API_KEY not set"
          fi
          if [ -z "${{ secrets.SUPABASE_URL }}" ]; then
            echo "Warning: SUPABASE_URL not set, traceability map features will be limited"
          fi

      - name: Run Docureco agent (PR Analysis)
        env:

          GITHUB_EVENT_PATH: ${{ github.event_path }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}


          DOCURECO_LLM_PROVIDER: ${{ vars.DOCURECO_LLM_PROVIDER || 'grok' }}
          DOCURECO_LLM_MODEL: ${{ vars.DOCURECO_LLM_MODEL || 'grok-3-mini-reasoning-high' }}
          GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
          GROK_BASE_URL: ${{ vars.GROK_BASE_URL || 'https://api.x.ai/v1' }}


          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_BASE_URL: ${{ vars.OPENAI_BASE_URL }}


          DOCURECO_LLM_TEMPERATURE: ${{ vars.DOCURECO_LLM_TEMPERATURE || '0.1' }}
          DOCURECO_LLM_MAX_TOKENS: ${{ vars.DOCURECO_LLM_MAX_TOKENS || '4000' }}
          DOCURECO_LLM_MAX_RETRIES: ${{ vars.DOCURECO_LLM_MAX_RETRIES || '3' }}
          DOCURECO_LLM_TIMEOUT: ${{ vars.DOCURECO_LLM_TIMEOUT || '120' }}


          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}


          LOG_LEVEL: ${{ vars.LOG_LEVEL || 'INFO' }}
          MAX_CONCURRENT_OPERATIONS: ${{ vars.MAX_CONCURRENT_OPERATIONS || '5' }}
          REQUEST_TIMEOUT_SECONDS: ${{ vars.REQUEST_TIMEOUT_SECONDS || '120' }}
          MAX_FILE_SIZE_MB: ${{ vars.MAX_FILE_SIZE_MB || '10' }}

        run: |
          cd agent
          python -m main
        timeout-minutes: 10

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: docureco-analyze-logs-${{ github.run_id }}
          path: |
            agent/*.log
            /tmp/docureco*.log
          retention-days: 7


  create-baseline-map:
    if: >
      github.event_name == 'workflow_dispatch' && inputs.action == 'create_baseline_map'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch || 'main' }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r agent/requirements.txt

      - name: Create baseline traceability map
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          TARGET_REPOSITORY: ${{ inputs.repository || github.repository }}
          TARGET_BRANCH: ${{ inputs.branch || 'main' }}
          LOG_LEVEL: ${{ vars.LOG_LEVEL || 'INFO' }}
        run: |
          cd agent
          python -m baseline_map_creator
        timeout-minutes: 30

      - name: Upload baseline map creation logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: docureco-baseline-creation-logs-${{ github.run_id }}
          path: |
            agent/*.log
            /tmp/docureco*.log
          retention-days: 7


  update-baseline-map:
    if: >
      (github.event_name == 'pull_request_target' && github.event.pull_request.merged == true) ||
      (github.event_name == 'workflow_dispatch' && inputs.action == 'update_baseline_map')
    runs-on: ubuntu-latest
    steps:
      - name: Validate PR merge status
        if: github.event_name == 'pull_request_target'
        run: |
          echo "Checking if PR was actually merged..."
          if [ "${{ github.event.pull_request.merged }}" != "true" ]; then
            echo "❌ PR was closed without merging. Skipping baseline map update."
            echo "PR #${{ github.event.pull_request.number }} status: closed (not merged)"
            exit 78
          else
            echo "✅ PR #${{ github.event.pull_request.number }} was successfully merged"
            echo "Target branch: ${{ github.event.pull_request.base.ref }}"
            echo "Merged commit: ${{ github.event.pull_request.merge_commit_sha }}"
          fi

      - name: Checkout merged code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.base.ref || inputs.branch || 'main' }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r agent/requirements.txt

      - name: Update baseline traceability map
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          TARGET_REPOSITORY: ${{ inputs.repository || github.repository }}
          TARGET_BRANCH: ${{ github.event.pull_request.base.ref || inputs.branch || 'main' }}
          MERGED_PR_NUMBER: ${{ github.event.pull_request.number }}
          LOG_LEVEL: ${{ vars.LOG_LEVEL || 'INFO' }}
        run: |
          cd agent
          python -m baseline_map_updater
        timeout-minutes: 20

      - name: Upload baseline map update logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: docureco-baseline-update-logs-${{ github.run_id }}
          path: |
            agent/*.log
            /tmp/docureco*.log
          retention-days: 7

      - name: Comment on merged PR
        if: github.event_name == 'pull_request_target' && github.event.pull_request.merged == true
        uses: actions/github-script@v7
        with:
          script: |
            const mergedAt = '${{ github.event.pull_request.merged_at }}';
            const baseBranch = '${{ github.event.pull_request.base.ref }}';
            const headBranch = '${{ github.event.pull_request.head.ref }}';

            github.rest.issues.createComment({
              issue_number: ${{ github.event.pull_request.number }},
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `🗺️ **Baseline Traceability Map Updated**\n\nThe baseline traceability map for branch \`${baseBranch}\` has been automatically updated to reflect the changes from this merged pull request.\n\n**Merge Details:**\n- **From:** \`${headBranch}\` → \`${baseBranch}\`\n- **Merged at:** ${mergedAt}\n- **Workflow run:** [View logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n\nThe updated baseline map will be used for future PR analysis and documentation recommendations.\n\n_Generated by Docureco Agent_`
            })
</file>

<file path=".github/workflows/update-baseline-map.yml">
name: 'Docureco Agent: Update Baseline Map'

on:
  workflow_dispatch:
    inputs:
      repository:
        description: 'Repository name (owner/repo)'
        required: true
        default: 'mhelmih/docureco'
      branch:
        description: 'Branch name to analyze'
        required: true
        default: 'main'
      update_strategy:
        description: 'Update strategy'
        required: true
        default: 'incremental'
        type: choice
        options:
          - incremental
          - full_refresh
          - merge
  schedule:

    - cron: '0 3 * * 0'

env:

  GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
  GROK_BASE_URL: ${{ secrets.GROK_BASE_URL }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}


  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}


  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  update-baseline-map:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        cd agent
        pip install -r requirements.txt

    - name: Determine repositories to update
      id: repos
      run: |
        if [ "${{ github.event_name }}" = "schedule" ]; then

          echo "mode=scheduled" >> $GITHUB_OUTPUT
          echo "repositories=[]" >> $GITHUB_OUTPUT
        else

          echo "mode=manual" >> $GITHUB_OUTPUT
          echo "repositories=[\"${{ github.event.inputs.repository }}\"]" >> $GITHUB_OUTPUT
          echo "branch=${{ github.event.inputs.branch }}" >> $GITHUB_OUTPUT
          echo "strategy=${{ github.event.inputs.update_strategy }}" >> $GITHUB_OUTPUT
        fi

    - name: Validate inputs (manual trigger only)
      if: github.event_name == 'workflow_dispatch'
      run: |
        echo "Repository: ${{ github.event.inputs.repository }}"
        echo "Branch: ${{ github.event.inputs.branch }}"
        echo "Update Strategy: ${{ github.event.inputs.update_strategy }}"


        if [[ ! "${{ github.event.inputs.repository }}" =~ ^[a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+$ ]]; then
          echo "Error: Repository must be in format 'owner/repo'"
          exit 1
        fi

    - name: Update baseline traceability map
      run: |
        cd agent
        python -c "
        import asyncio
        import os
        import sys
        import json
        from workflows.baseline_map_creator import create_baseline_map_creator
        from database.baseline_map_repository import BaselineMapRepository

        async def update_single_repository(repository, branch='main', strategy='incremental'):
            try:
                print(f'🔄 Updating baseline map for {repository}:{branch}')
                print(f'📋 Strategy: {strategy}')

                # Check if baseline map exists
                repo_client = BaselineMapRepository()
                existing_map = await repo_client.get_baseline_map(repository, branch)

                if not existing_map:
                    print(f'❌ No existing baseline map found for {repository}:{branch}')
                    print('💡 Tip: Run the \"Create Baseline Map\" workflow first')
                    return False

                print(f'✅ Found existing baseline map with {len(existing_map.requirements)} requirements')

                # Create workflow instance
                creator = create_baseline_map_creator()

                if strategy == 'full_refresh':
                    print('🔄 Performing full refresh...')
                    # Delete existing and recreate
                    await repo_client.delete_baseline_map(repository, branch)
                    result = await creator.execute(repository, branch)

                elif strategy == 'incremental':
                    print('📈 Performing incremental update...')
                    # Compare current repo state with existing baseline
                    result = await creator.execute(repository, branch)

                    # TODO: Implement incremental update logic
                    # - Compare new elements with existing
                    # - Update only changed/new elements
                    # - Preserve existing traceability links where possible

                elif strategy == 'merge':
                    print('🔀 Performing merge update...')
                    # Merge new elements with existing baseline
                    result = await creator.execute(repository, branch)

                    # TODO: Implement merge logic
                    # - Add new elements without removing existing
                    # - Update modified elements
                    # - Create new traceability links

                # Print results
                print('\\n📊 Baseline Map Update Results:')
                print(f'Repository: {result.repository}:{result.branch}')
                print(f'Requirements: {len(result.requirements)}')
                print(f'Design Elements: {len(result.design_elements)}')
                print(f'Code Components: {len(result.code_components)}')
                print(f'Traceability Links: {len(result.traceability_links)}')

                if result.errors:
                    print('\\n⚠️  Errors encountered:')
                    for error in result.errors:
                        print(f'  - {error}')
                    return False

                print(f'\\n✅ Baseline map update completed: {result.current_step}')
                return True

            except Exception as e:
                print(f'❌ Baseline map update failed for {repository}: {str(e)}')
                return False

        async def update_scheduled_repositories():
            '''Update all repositories that have existing baseline maps'''
            try:
                print('🕐 Running scheduled baseline map updates...')

                # Get all repositories with existing baseline maps
                repo_client = BaselineMapRepository()
                repositories = await repo_client.get_all_baseline_map_repositories()

                if not repositories:
                    print('ℹ️  No repositories with baseline maps found')
                    return True

                print(f'📊 Found {len(repositories)} repositories to update')

                success_count = 0
                for repo_info in repositories:
                    repository = repo_info['repository']
                    branch = repo_info['branch']

                    success = await update_single_repository(repository, branch, 'incremental')
                    if success:
                        success_count += 1

                    # Brief pause between repositories to avoid rate limits
                    await asyncio.sleep(2)

                print(f'\\n📈 Update Summary: {success_count}/{len(repositories)} successful')
                return success_count == len(repositories)

            except Exception as e:
                print(f'❌ Scheduled update failed: {str(e)}')
                return False

        async def main():
            try:
                mode = '${{ steps.repos.outputs.mode }}'

                if mode == 'scheduled':
                    success = await update_scheduled_repositories()
                else:
                    # Manual update
                    repository = '${{ github.event.inputs.repository }}'
                    branch = '${{ github.event.inputs.branch }}'
                    strategy = '${{ github.event.inputs.update_strategy }}'

                    success = await update_single_repository(repository, branch, strategy)

                if not success:
                    sys.exit(1)

            except Exception as e:
                print(f'❌ Update process failed: {str(e)}')
                sys.exit(1)

        asyncio.run(main())
        "

    - name: Upload update artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: baseline-map-update-${{ github.run_id }}
        path: |
          agent/logs/
          agent/output/
        retention-days: 30

    - name: Create summary
      if: always()
      run: |
        echo "## 🔄 Baseline Map Update Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "**Trigger:** Scheduled (Weekly)" >> $GITHUB_STEP_SUMMARY
          echo "**Scope:** All repositories with existing baseline maps" >> $GITHUB_STEP_SUMMARY
        else
          echo "**Trigger:** Manual" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** \`${{ github.event.inputs.repository }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** \`${{ github.event.inputs.branch }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Strategy:** \`${{ github.event.inputs.update_strategy }}\`" >> $GITHUB_STEP_SUMMARY
        fi

        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "
        echo "- Started by: @${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "- Workflow: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
        echo "- Run ID: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "
        echo "- **Incremental**: Update only changed elements (recommended)" >> $GITHUB_STEP_SUMMARY
        echo "- **Full Refresh**: Complete regeneration of baseline map" >> $GITHUB_STEP_SUMMARY
        echo "- **Merge**: Add new elements without removing existing ones" >> $GITHUB_STEP_SUMMARY
</file>

<file path="agent/config/__init__.py">
__all__ = [
</file>

<file path="agent/config/llm_config.py">
class LLMProvider(str, Enum)
⋮----
GROK = "grok"
OPENAI = "openai"
⋮----
class LLMConfig(BaseModel)
⋮----
provider: LLMProvider = Field(default=LLMProvider.GROK)
model_name: str = Field(default="grok-3-mini-reasoning-high")
api_key: Optional[str] = Field(default=None)
base_url: Optional[str] = Field(default=None)
temperature: float = Field(default=0.1, ge=0.0, le=2.0)
max_tokens: int = Field(default=4000, gt=0)
max_retries: int = Field(default=3, ge=0)
request_timeout: int = Field(default=120, gt=0)
⋮----
top_p: float = Field(default=0.9, ge=0.0, le=1.0)
frequency_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
presence_penalty: float = Field(default=0.0, ge=-2.0, le=2.0)
⋮----
class TaskSpecificConfig(BaseModel)
⋮----
code_analysis: Dict[str, Any] = Field(default_factory=lambda: {
⋮----
traceability_mapping: Dict[str, Any] = Field(default_factory=lambda: {
⋮----
impact_assessment: Dict[str, Any] = Field(default_factory=lambda: {
⋮----
recommendation_generation: Dict[str, Any] = Field(default_factory=lambda: {
⋮----
def get_llm_config() -> LLMConfig
⋮----
provider = LLMProvider(os.getenv("DOCURECO_LLM_PROVIDER", "grok"))
⋮----
config = LLMConfig(
⋮----
def get_task_config() -> TaskSpecificConfig
⋮----
__all__ = ["LLMProvider", "LLMConfig", "TaskSpecificConfig", "get_llm_config", "get_task_config"]
</file>

<file path="agent/database/migrations/001_create_baseline_map_tables.sql">
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";


CREATE TABLE IF NOT EXISTS baseline_maps (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    repository TEXT NOT NULL,
    branch TEXT NOT NULL DEFAULT 'main',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),


    UNIQUE(repository, branch)
);


CREATE TABLE IF NOT EXISTS requirements (
    id TEXT NOT NULL,
    baseline_map_id UUID NOT NULL REFERENCES baseline_maps(id) ON DELETE CASCADE,
    title TEXT NOT NULL,
    description TEXT NOT NULL,
    type TEXT NOT NULL,
    priority TEXT NOT NULL DEFAULT 'Medium',
    section TEXT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    PRIMARY KEY (id, baseline_map_id)
);


CREATE TABLE IF NOT EXISTS design_elements (
    id TEXT NOT NULL,
    baseline_map_id UUID NOT NULL REFERENCES baseline_maps(id) ON DELETE CASCADE,
    name TEXT NOT NULL,
    description TEXT NOT NULL,
    type TEXT NOT NULL,
    section TEXT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    PRIMARY KEY (id, baseline_map_id)
);


CREATE TABLE IF NOT EXISTS code_components (
    id TEXT NOT NULL,
    baseline_map_id UUID NOT NULL REFERENCES baseline_maps(id) ON DELETE CASCADE,
    path TEXT NOT NULL,
    type TEXT NOT NULL,
    name TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    PRIMARY KEY (id, baseline_map_id)
);


CREATE TABLE IF NOT EXISTS traceability_links (
    id TEXT NOT NULL,
    baseline_map_id UUID NOT NULL REFERENCES baseline_maps(id) ON DELETE CASCADE,
    source_type TEXT NOT NULL,
    source_id TEXT NOT NULL,
    target_type TEXT NOT NULL,
    target_id TEXT NOT NULL,
    relationship_type TEXT NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    PRIMARY KEY (id, baseline_map_id)
);


CREATE INDEX IF NOT EXISTS idx_baseline_maps_repository ON baseline_maps(repository);
CREATE INDEX IF NOT EXISTS idx_baseline_maps_repository_branch ON baseline_maps(repository, branch);
CREATE INDEX IF NOT EXISTS idx_baseline_maps_updated_at ON baseline_maps(updated_at);

CREATE INDEX IF NOT EXISTS idx_requirements_baseline_map_id ON requirements(baseline_map_id);
CREATE INDEX IF NOT EXISTS idx_requirements_type ON requirements(type);
CREATE INDEX IF NOT EXISTS idx_requirements_priority ON requirements(priority);

CREATE INDEX IF NOT EXISTS idx_design_elements_baseline_map_id ON design_elements(baseline_map_id);
CREATE INDEX IF NOT EXISTS idx_design_elements_type ON design_elements(type);

CREATE INDEX IF NOT EXISTS idx_code_components_baseline_map_id ON code_components(baseline_map_id);
CREATE INDEX IF NOT EXISTS idx_code_components_path ON code_components(path);
CREATE INDEX IF NOT EXISTS idx_code_components_type ON code_components(type);

CREATE INDEX IF NOT EXISTS idx_traceability_links_baseline_map_id ON traceability_links(baseline_map_id);
CREATE INDEX IF NOT EXISTS idx_traceability_links_source ON traceability_links(source_type, source_id);
CREATE INDEX IF NOT EXISTS idx_traceability_links_target ON traceability_links(target_type, target_id);
CREATE INDEX IF NOT EXISTS idx_traceability_links_relationship ON traceability_links(relationship_type);


CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;


CREATE TRIGGER update_baseline_maps_updated_at
    BEFORE UPDATE ON baseline_maps
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_requirements_updated_at
    BEFORE UPDATE ON requirements
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_design_elements_updated_at
    BEFORE UPDATE ON design_elements
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_code_components_updated_at
    BEFORE UPDATE ON code_components
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_traceability_links_updated_at
    BEFORE UPDATE ON traceability_links
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();


ALTER TABLE baseline_maps ENABLE ROW LEVEL SECURITY;
ALTER TABLE requirements ENABLE ROW LEVEL SECURITY;
ALTER TABLE design_elements ENABLE ROW LEVEL SECURITY;
ALTER TABLE code_components ENABLE ROW LEVEL SECURITY;
ALTER TABLE traceability_links ENABLE ROW LEVEL SECURITY;



CREATE POLICY "Allow service role full access on baseline_maps" ON baseline_maps
    FOR ALL USING (auth.role() = 'service_role');

CREATE POLICY "Allow service role full access on requirements" ON requirements
    FOR ALL USING (auth.role() = 'service_role');

CREATE POLICY "Allow service role full access on design_elements" ON design_elements
    FOR ALL USING (auth.role() = 'service_role');

CREATE POLICY "Allow service role full access on code_components" ON code_components
    FOR ALL USING (auth.role() = 'service_role');

CREATE POLICY "Allow service role full access on traceability_links" ON traceability_links
    FOR ALL USING (auth.role() = 'service_role');



/*
CREATE POLICY "Allow authenticated read access on baseline_maps" ON baseline_maps
    FOR SELECT USING (auth.role() = 'authenticated');

CREATE POLICY "Allow authenticated read access on requirements" ON requirements
    FOR SELECT USING (auth.role() = 'authenticated');

CREATE POLICY "Allow authenticated read access on design_elements" ON design_elements
    FOR SELECT USING (auth.role() = 'authenticated');

CREATE POLICY "Allow authenticated read access on code_components" ON code_components
    FOR SELECT USING (auth.role() = 'authenticated');

CREATE POLICY "Allow authenticated read access on traceability_links" ON traceability_links
    FOR SELECT USING (auth.role() = 'authenticated');
*/


/*
INSERT INTO baseline_maps (repository, branch) VALUES ('mhelmih/docureco', 'main');

INSERT INTO requirements (id, baseline_map_id, title, description, type, priority, section)
SELECT 'REQ-001', id, 'User Authentication', 'System must authenticate users', 'Functional', 'High', '3.1.1'
FROM baseline_maps WHERE repository = 'mhelmih/docureco' AND branch = 'main';

INSERT INTO design_elements (id, baseline_map_id, name, description, type, section)
SELECT 'DE-001', id, 'AuthService', 'Authentication service class', 'Class', '4.2.1'
FROM baseline_maps WHERE repository = 'mhelmih/docureco' AND branch = 'main';

INSERT INTO code_components (id, baseline_map_id, path, type, name)
SELECT 'CC-001', id, 'src/auth/AuthService.java', 'Class', 'AuthService'
FROM baseline_maps WHERE repository = 'mhelmih/docureco' AND branch = 'main';

INSERT INTO traceability_links (id, baseline_map_id, source_type, source_id, target_type, target_id, relationship_type)
SELECT 'TL-001', id, 'Requirement', 'REQ-001', 'DesignElement', 'DE-001', 'implements'
FROM baseline_maps WHERE repository = 'mhelmih/docureco' AND branch = 'main';

INSERT INTO traceability_links (id, baseline_map_id, source_type, source_id, target_type, target_id, relationship_type)
SELECT 'TL-002', id, 'DesignElement', 'DE-001', 'CodeComponent', 'CC-001', 'realizes'
FROM baseline_maps WHERE repository = 'mhelmih/docureco' AND branch = 'main';
*/
</file>

<file path="agent/database/migrations/002_add_vector_embeddings.sql">
CREATE EXTENSION IF NOT EXISTS vector;


ALTER TABLE requirements
ADD COLUMN IF NOT EXISTS title_embedding vector(1536),
ADD COLUMN IF NOT EXISTS description_embedding vector(1536),
ADD COLUMN IF NOT EXISTS combined_embedding vector(1536);


ALTER TABLE design_elements
ADD COLUMN IF NOT EXISTS name_embedding vector(1536),
ADD COLUMN IF NOT EXISTS description_embedding vector(1536),
ADD COLUMN IF NOT EXISTS combined_embedding vector(1536);


ALTER TABLE code_components
ADD COLUMN IF NOT EXISTS path_embedding vector(1536),
ADD COLUMN IF NOT EXISTS name_embedding vector(1536);


CREATE INDEX IF NOT EXISTS idx_requirements_title_embedding
ON requirements USING ivfflat (title_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_requirements_description_embedding
ON requirements USING ivfflat (description_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_requirements_combined_embedding
ON requirements USING ivfflat (combined_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_design_elements_name_embedding
ON design_elements USING ivfflat (name_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_design_elements_description_embedding
ON design_elements USING ivfflat (description_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_design_elements_combined_embedding
ON design_elements USING ivfflat (combined_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_code_components_path_embedding
ON code_components USING ivfflat (path_embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_code_components_name_embedding
ON code_components USING ivfflat (name_embedding vector_cosine_ops)
WITH (lists = 100);


CREATE OR REPLACE FUNCTION find_similar_requirements(
    query_embedding vector(1536),
    similarity_threshold float DEFAULT 0.7,
    max_results int DEFAULT 10,
    target_baseline_map_id uuid DEFAULT NULL
)
RETURNS TABLE (
    id text,
    title text,
    description text,
    similarity float
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        r.id,
        r.title,
        r.description,
        1 - (r.combined_embedding <=> query_embedding) AS similarity
    FROM requirements r
    WHERE
        (target_baseline_map_id IS NULL OR r.baseline_map_id = target_baseline_map_id)
        AND r.combined_embedding IS NOT NULL
        AND (1 - (r.combined_embedding <=> query_embedding)) >= similarity_threshold
    ORDER BY r.combined_embedding <=> query_embedding
    LIMIT max_results;
END;
$$ LANGUAGE plpgsql;


CREATE OR REPLACE FUNCTION find_similar_design_elements(
    query_embedding vector(1536),
    similarity_threshold float DEFAULT 0.7,
    max_results int DEFAULT 10,
    target_baseline_map_id uuid DEFAULT NULL
)
RETURNS TABLE (
    id text,
    name text,
    description text,
    similarity float
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        de.id,
        de.name,
        de.description,
        1 - (de.combined_embedding <=> query_embedding) AS similarity
    FROM design_elements de
    WHERE
        (target_baseline_map_id IS NULL OR de.baseline_map_id = target_baseline_map_id)
        AND de.combined_embedding IS NOT NULL
        AND (1 - (de.combined_embedding <=> query_embedding)) >= similarity_threshold
    ORDER BY de.combined_embedding <=> query_embedding
    LIMIT max_results;
END;
$$ LANGUAGE plpgsql;


CREATE OR REPLACE FUNCTION find_similar_code_components(
    query_embedding vector(1536),
    similarity_threshold float DEFAULT 0.7,
    max_results int DEFAULT 10,
    target_baseline_map_id uuid DEFAULT NULL
)
RETURNS TABLE (
    id text,
    path text,
    name text,
    similarity float
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        cc.id,
        cc.path,
        cc.name,
        1 - (COALESCE(cc.name_embedding, cc.path_embedding) <=> query_embedding) AS similarity
    FROM code_components cc
    WHERE
        (target_baseline_map_id IS NULL OR cc.baseline_map_id = target_baseline_map_id)
        AND (cc.name_embedding IS NOT NULL OR cc.path_embedding IS NOT NULL)
        AND (1 - (COALESCE(cc.name_embedding, cc.path_embedding) <=> query_embedding)) >= similarity_threshold
    ORDER BY COALESCE(cc.name_embedding, cc.path_embedding) <=> query_embedding
    LIMIT max_results;
END;
$$ LANGUAGE plpgsql;





COMMENT ON COLUMN requirements.title_embedding IS 'Vector embedding of requirement title for semantic search';
COMMENT ON COLUMN requirements.description_embedding IS 'Vector embedding of requirement description for semantic search';
COMMENT ON COLUMN requirements.combined_embedding IS 'Vector embedding of combined title + description for semantic search';
COMMENT ON COLUMN design_elements.name_embedding IS 'Vector embedding of design element name for semantic search';
COMMENT ON COLUMN design_elements.description_embedding IS 'Vector embedding of design element description for semantic search';
COMMENT ON COLUMN design_elements.combined_embedding IS 'Vector embedding of combined name + description for semantic search';
COMMENT ON COLUMN code_components.path_embedding IS 'Vector embedding of code component path for semantic search';
COMMENT ON COLUMN code_components.name_embedding IS 'Vector embedding of code component name for semantic search';
</file>

<file path="agent/database/__init__.py">
logger = logging.getLogger(__name__)
⋮----
def create_database_client() -> Optional[SupabaseClient]
⋮----
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
⋮----
database_url = os.getenv("DATABASE_URL")
⋮----
def create_baseline_map_repository() -> BaselineMapRepository
⋮----
db_client = create_database_client()
⋮----
def create_vector_search_repository() -> VectorSearchRepository
⋮----
__all__ = [
</file>

<file path="agent/database/baseline_map_repository.py">
logger = logging.getLogger(__name__)
⋮----
class BaselineMapRepository
⋮----
def __init__(self, supabase_client: Optional[SupabaseClient] = None)
⋮----
async def get_baseline_map(self, repository: str, branch: str = "main") -> Optional[BaselineMapModel]
⋮----
map_data = await self.client.get_baseline_map(repository, branch)
⋮----
baseline_map = BaselineMapModel(
⋮----
async def save_baseline_map(self, baseline_map: BaselineMapModel) -> bool
⋮----
map_data = {
⋮----
baseline_map = await self.get_baseline_map(repository, branch)
⋮----
code_component_id = None
⋮----
code_component_id = component.id
⋮----
affected_elements = []
⋮----
target_element = self._find_element_by_id(
⋮----
def _find_element_by_id(self, baseline_map: BaselineMapModel, element_type: str, element_id: str) -> Optional[Dict[str, Any]]
⋮----
updated = False
⋮----
updated = True
⋮----
async def check_repository_exists(self, repository: str, branch: str = "main") -> bool
⋮----
async def get_repository_statistics(self, repository: str, branch: str = "main") -> Dict[str, int]
⋮----
__all__ = ["BaselineMapRepository"]
</file>

<file path="agent/database/supabase_client.py">
logger = logging.getLogger(__name__)
⋮----
class SupabaseClient
⋮----
def __init__(self, url: Optional[str] = None, key: Optional[str] = None)
⋮----
options = ClientOptions(
⋮----
def _test_connection(self) -> None
⋮----
response = self.client.table("repositories").select("id").limit(1).execute()
⋮----
async def get_baseline_map(self, repository: str, branch: str = "main") -> Optional[Dict[str, Any]]
⋮----
# Get baseline map record
response = self.client.table("baseline_maps").select("""
⋮----
baseline_map = response.data[0]
⋮----
async def save_baseline_map(self, baseline_map: Dict[str, Any]) -> bool
⋮----
repository = baseline_map["repository"]
branch = baseline_map.get("branch", "main")
⋮----
existing = await self.get_baseline_map(repository, branch)
⋮----
baseline_map_id = existing["id"]
⋮----
async def _create_baseline_map(self, baseline_map: Dict[str, Any]) -> str
⋮----
map_response = self.client.table("baseline_maps").insert({
⋮----
baseline_map_id = map_response.data[0]["id"]
⋮----
async def _update_baseline_map(self, baseline_map_id: str, baseline_map: Dict[str, Any]) -> None
⋮----
async def _insert_requirements(self, baseline_map_id: str, requirements: List[Dict[str, Any]]) -> None
⋮----
records = []
⋮----
async def _insert_design_elements(self, baseline_map_id: str, design_elements: List[Dict[str, Any]]) -> None
⋮----
async def _insert_code_components(self, baseline_map_id: str, code_components: List[Dict[str, Any]]) -> None
⋮----
async def _insert_traceability_links(self, baseline_map_id: str, links: List[Dict[str, Any]]) -> None
⋮----
map_response = self.client.table("baseline_maps").select("id").eq(
⋮----
response = self.client.table("traceability_links").select("*").eq(
⋮----
def create_supabase_client(url: Optional[str] = None, key: Optional[str] = None) -> SupabaseClient
⋮----
__all__ = ["SupabaseClient", "create_supabase_client"]
</file>

<file path="agent/database/vector_search_repository.py">
logger = logging.getLogger(__name__)
⋮----
class VectorSearchRepository
⋮----
query_embedding = await self.embedding_client.embed_text(query_text)
⋮----
baseline_map = await self.supabase_client.get_baseline_map(repository, branch)
⋮----
baseline_map_id = baseline_map["id"]
⋮----
response = self.supabase_client.client.rpc(
⋮----
results = response.data or []
⋮----
change_embedding = await self.embedding_client.embed_code_change_context(
⋮----
tasks = [
⋮----
results = await asyncio.gather(*tasks, return_exceptions=True)
⋮----
requirements = results[0].data if not isinstance(results[0], Exception) else []
design_elements = results[1].data if not isinstance(results[1], Exception) else []
code_components = results[2].data if not isinstance(results[2], Exception) else []
⋮----
db_baseline_map = await self.supabase_client.get_baseline_map(
⋮----
baseline_map_id = db_baseline_map["id"]
⋮----
requirements = baseline_map.get("requirements", [])
⋮----
design_elements = baseline_map.get("design_elements", [])
⋮----
code_components = baseline_map.get("code_components", [])
⋮----
async def _update_requirement_embeddings(self, baseline_map_id: str, requirements: List[Dict[str, Any]]) -> None
⋮----
embeddings = await self.embedding_client.embed_requirement(req)
⋮----
async def _update_design_element_embeddings(self, baseline_map_id: str, design_elements: List[Dict[str, Any]]) -> None
⋮----
embeddings = await self.embedding_client.embed_design_element(elem)
⋮----
async def _update_code_component_embeddings(self, baseline_map_id: str, code_components: List[Dict[str, Any]]) -> None
⋮----
embeddings = await self.embedding_client.embed_code_component(comp)
⋮----
update_data = {"path_embedding": embeddings["path_embedding"]}
⋮----
__all__ = ["VectorSearchRepository", "create_vector_search_repository"]
</file>

<file path="agent/llm/__init__.py">
__all__ = [
</file>

<file path="agent/llm/embedding_client.py">
logger = logging.getLogger(__name__)
⋮----
class DocurecoEmbeddingClient
⋮----
# Initialize LangChain OpenAI embeddings
⋮----
async def embed_text(self, text: str) -> List[float]
⋮----
# Return zero vector for empty text
⋮----
# Use LangChain's async method
embedding = await self.embeddings.aembed_query(text.strip())
⋮----
async def embed_texts(self, texts: List[str]) -> List[List[float]]
⋮----
embeddings = []
⋮----
batch = texts[i:i + self.batch_size]
⋮----
filtered_batch = [text.strip() for text in batch if text and text.strip()]
⋮----
batch_embeddings = await self.embeddings.aembed_documents(filtered_batch)
⋮----
async def embed_requirement(self, requirement: Dict[str, Any]) -> Dict[str, List[float]]
⋮----
title = requirement.get("title", "")
description = requirement.get("description", "")
combined = f"{title}. {description}".strip()
⋮----
embeddings = await self.embed_texts([title, description, combined])
⋮----
async def embed_design_element(self, design_element: Dict[str, Any]) -> Dict[str, List[float]]
⋮----
name = design_element.get("name", "")
description = design_element.get("description", "")
element_type = design_element.get("type", "")
combined = f"{name} ({element_type}). {description}".strip()
⋮----
embeddings = await self.embed_texts([name, description, combined])
⋮----
async def embed_code_component(self, code_component: Dict[str, Any]) -> Dict[str, List[float]]
⋮----
path = code_component.get("path", "")
name = code_component.get("name", "")
⋮----
texts_to_embed = []
⋮----
embeddings = await self.embed_texts(texts_to_embed)
⋮----
result = {"path_embedding": embeddings[0]}
⋮----
context_parts = []
⋮----
patch_lines = patch.split('\n')
meaningful_lines = []
⋮----
context = ". ".join(context_parts)
⋮----
def get_model_info(self) -> Dict[str, Any]
⋮----
__all__ = ["DocurecoEmbeddingClient", "create_embedding_client"]
</file>

<file path="agent/llm/llm_client.py">
logger = logging.getLogger(__name__)
⋮----
@dataclass
class LLMResponse
⋮----
content: str
metadata: Dict[str, Any]
model_used: str
tokens_used: Optional[int] = None
cost: Optional[float] = None
⋮----
class DocurecoLLMClient
⋮----
def __init__(self, config: Optional[LLMConfig] = None)
⋮----
def _initialize_llm(self) -> BaseLanguageModel
⋮----
def _initialize_grok(self) -> ChatOpenAI
⋮----
def _initialize_openai(self) -> ChatOpenAI
⋮----
llm = self._configure_for_task(task_type, **kwargs)
⋮----
messages = []
⋮----
response = await llm.ainvoke(messages)
⋮----
parser = JsonOutputParser()
parsed_content = parser.parse(response.content)
⋮----
parsed_content = response.content
⋮----
def _configure_for_task(self, task_type: Optional[str], **kwargs) -> BaseLanguageModel
⋮----
task_config = getattr(self.task_config, task_type, {})
⋮----
def create_prompt_template(self, template_name: str) -> ChatPromptTemplate
⋮----
templates = {
⋮----
def _get_code_classification_template(self) -> ChatPromptTemplate
⋮----
system_template = """You are an expert software analyst for the Docureco system. Your task is to classify code changes according to the 4W framework:
⋮----
human_template = """Analyze this code change:
⋮----
def _get_traceability_mapping_template(self) -> ChatPromptTemplate
⋮----
system_template = """You are an expert software architect for the Docureco system. Your task is to establish traceability mappings between code components, design elements, and requirements.
⋮----
human_template = """Create traceability mappings for:
⋮----
def _get_impact_assessment_template(self) -> ChatPromptTemplate
⋮----
system_template = """You are an expert software analyst for the Docureco system. Your task is to assess the impact of code changes on documentation (SRS and SDD).
⋮----
human_template = """Assess the impact of these findings:
⋮----
def _get_recommendation_generation_template(self) -> ChatPromptTemplate
⋮----
system_template = """You are an expert technical writer for the Docureco system. Your task is to generate specific, actionable recommendations for updating SRS and SDD documentation.
⋮----
human_template = """Generate documentation update recommendations for:
⋮----
def create_llm_client(config: Optional[LLMConfig] = None) -> DocurecoLLMClient
⋮----
__all__ = ["DocurecoLLMClient", "LLMResponse", "create_llm_client"]
</file>

<file path="agent/models/__init__.py">
__all__ = [
</file>

<file path="agent/models/docureco_models.py">
class ChangeType(str, Enum)
⋮----
ADDITION = "Addition"
DELETION = "Deletion"
MODIFICATION = "Modification"
RENAME = "Rename"
⋮----
class ChangeScope(str, Enum)
⋮----
FUNCTION_METHOD = "Function/Method"
CLASS_INTERFACE = "Class/Interface/Struct/Type"
MODULE_PACKAGE = "Module/Package/Namespace"
FILE = "File"
API_CONTRACT = "API Contract"
CONFIGURATION = "Configuration"
DEPENDENCIES = "Dependencies"
BUILD_SCRIPTS = "Build Scripts"
INFRASTRUCTURE = "Infrastructure Code"
TEST_CODE = "Test Code"
DOCUMENTATION = "Documentation"
CROSS_CUTTING = "Cross-cutting Concerns"
⋮----
class ChangeNature(str, Enum)
⋮----
NEW_FEATURE = "New Feature"
FEATURE_ENHANCEMENT = "Feature Enhancement"
BUG_FIX = "Bug Fix"
SECURITY_FIX = "Security Fix"
⋮----
REFACTORING = "Refactoring"
PERFORMANCE_OPTIMIZATION = "Performance Optimization"
CODE_STYLE = "Code Style/Formatting"
TECH_DEBT_REDUCTION = "Technical Debt Reduction"
READABILITY_IMPROVEMENT = "Readability Improvement"
ERROR_HANDLING = "Error Handling Improvement"
⋮----
DEPENDENCY_MANAGEMENT = "Dependency Management"
BUILD_IMPROVEMENT = "Build Process Improvement"
TOOLING_CONFIG = "Tooling Configuration"
⋮----
API_CHANGE = "API Change"
EXTERNAL_INTEGRATION = "External System Integration"
⋮----
DOCUMENTATION_UPDATE = "Documentation Update"
UI_UX_ADJUSTMENT = "UI/UX Adjustment"
STATIC_CONTENT = "Static Content Update"
⋮----
CODE_DEPRECATION = "Code Deprecation/Removal"
REVERT = "Revert"
MERGE_CONFLICT = "Merge Conflict Resolution"
LICENSE_UPDATE = "License Update"
⋮----
EXPERIMENTAL = "Experimental"
CHORE = "Chore"
OTHER = "Other"
⋮----
class ChangeVolume(str, Enum)
⋮----
TRIVIAL = "Trivial"
SMALL = "Small"
MEDIUM = "Medium"
LARGE = "Large"
VERY_LARGE = "Very Large"
⋮----
class TraceabilityStatus(str, Enum)
⋮----
GAP = "Gap"
OUTDATED = "Outdated"
⋮----
ANOMALY_ADDITION_MAPPED = "Anomaly (addition mapped)"
ANOMALY_DELETION_UNMAPPED = "Anomaly (deletion unmapped)"
ANOMALY_MODIFICATION_UNMAPPED = "Anomaly (modification unmapped)"
ANOMALY_RENAME_UNMAPPED = "Anomaly (rename unmapped)"
⋮----
class FindingType(str, Enum)
⋮----
STANDARD_IMPACT = "Standard_Impact"
OUTDATED_DOCUMENTATION = "Outdated_Documentation"
DOCUMENTATION_GAP = "Documentation_Gap"
TRACEABILITY_ANOMALY = "Traceability_Anomaly"
⋮----
class Likelihood(str, Enum)
⋮----
VERY_LIKELY = "Very Likely"
LIKELY = "Likely"
POSSIBLY = "Possibly"
UNLIKELY = "Unlikely"
⋮----
class Severity(str, Enum)
⋮----
NONE = "None"
⋮----
MINOR = "Minor"
MODERATE = "Moderate"
MAJOR = "Major"
FUNDAMENTAL = "Fundamental"
⋮----
class TracePathType(str, Enum)
⋮----
DIRECT = "Direct"
INDIRECT = "Indirect"
⋮----
class ImpactSeverity(str, Enum)
⋮----
LOW = "Low"
⋮----
HIGH = "High"
⋮----
class RecommendationType(str, Enum)
⋮----
CREATE = "CREATE"
UPDATE = "UPDATE"
DELETE = "DELETE"
REVIEW = "REVIEW"
⋮----
class RecommendationStatus(str, Enum)
⋮----
PENDING = "PENDING"
IN_PROGRESS = "IN_PROGRESS"
COMPLETED = "COMPLETED"
REJECTED = "REJECTED"
⋮----
@dataclass
class CodeChangeClassification
⋮----
file: str
type: str
scope: str
nature: str
volume: str
reasoning: str
commit_hash: str
patch: str
⋮----
@dataclass
class LogicalChangeSet
⋮----
id: str
description: str
classifications: List[CodeChangeClassification]
commit_messages: List[str]
⋮----
@dataclass
class ImpactFindings
⋮----
finding_type: str
affected_element_type: str
affected_element_id: str
source_change_set_id: str
trace_path_type: Optional[str] = None
likelihood: str = "Possibly"
severity: str = "Minor"
⋮----
@dataclass
class DocumentationRecommendation
⋮----
finding_id: str
recommendation_text: str
action_type: str
priority: str
affected_document: str
affected_section: str
⋮----
class RequirementModel(BaseModel)
⋮----
id: str = Field(..., description="Unique requirement identifier")
title: str = Field(..., description="Requirement title")
description: str = Field(..., description="Requirement description")
type: str = Field(..., description="Functional or Non-functional")
priority: str = Field(default="Medium", description="Requirement priority")
section: str = Field(..., description="SRS section containing this requirement")
⋮----
class DesignElementModel(BaseModel)
⋮----
id: str = Field(..., description="Unique design element identifier")
name: str = Field(..., description="Design element name")
description: str = Field(..., description="Design element description")
type: str = Field(..., description="Type of design element (class, module, component, etc.)")
section: str = Field(..., description="SDD section containing this element")
⋮----
class CodeComponentModel(BaseModel)
⋮----
id: str = Field(..., description="Unique code component identifier")
path: str = Field(..., description="File path or component path")
type: str = Field(..., description="Type of component (file, class, function)")
name: Optional[str] = Field(None, description="Component name if applicable")
⋮----
class TraceabilityLinkModel(BaseModel)
⋮----
id: str = Field(..., description="Unique link identifier")
source_type: str = Field(..., description="Source artifact type")
source_id: str = Field(..., description="Source artifact ID")
target_type: str = Field(..., description="Target artifact type")
target_id: str = Field(..., description="Target artifact ID")
relationship_type: str = Field(..., description="Type of relationship")
created_at: datetime = Field(default_factory=datetime.now)
updated_at: datetime = Field(default_factory=datetime.now)
⋮----
class BaselineMapModel(BaseModel)
⋮----
requirements: List[RequirementModel] = Field(default_factory=list)
design_elements: List[DesignElementModel] = Field(default_factory=list)
code_components: List[CodeComponentModel] = Field(default_factory=list)
traceability_links: List[TraceabilityLinkModel] = Field(default_factory=list)
⋮----
repository: str = Field(..., description="Repository this map belongs to")
branch: str = Field(default="main", description="Branch this map represents")
⋮----
class PREventModel(BaseModel)
⋮----
pr_number: int
repository: str
action: str
base_ref: str
head_ref: str
base_sha: str
head_sha: str
title: str
body: Optional[str] = None
author: str
created_at: datetime
updated_at: datetime
⋮----
class CommitModel(BaseModel)
⋮----
sha: str
message: str
⋮----
timestamp: datetime
changed_files: List[str] = Field(default_factory=list)
⋮----
class FileChangeModel(BaseModel)
⋮----
filename: str
status: str
additions: int = 0
deletions: int = 0
patch: Optional[str] = None
previous_filename: Optional[str] = None
⋮----
class ClassificationResponseModel(BaseModel)
⋮----
classification: CodeChangeClassification
timestamp: datetime = Field(default_factory=datetime.now)
⋮----
class RecommendationResponseModel(BaseModel)
⋮----
recommendations: List[DocumentationRecommendation]
total_findings: int
high_priority_count: int
processing_time_seconds: float
⋮----
class WorkflowStatusModel(BaseModel)
⋮----
current_step: str
progress_percentage: float = 0.0
errors: List[str] = Field(default_factory=list)
started_at: datetime = Field(default_factory=datetime.now)
completed_at: Optional[datetime] = None
⋮----
class ImpactAnalysisResultModel(BaseModel)
⋮----
element_type: str = Field(..., description="Type of impacted element")
element_id: str = Field(..., description="ID of impacted element")
impact_reason: str = Field(..., description="Reason for impact")
likelihood: float = Field(..., ge=0.0, le=1.0, description="Likelihood score (0-1)")
severity: ImpactSeverity = Field(..., description="Impact severity")
change_details: Dict[str, Any] = Field(default_factory=dict, description="Details about the change")
⋮----
class DocumentationRecommendationModel(BaseModel)
⋮----
target_document: str = Field(..., description="Target document to update")
section: str = Field(..., description="Specific section to update")
recommendation_type: RecommendationType = Field(..., description="Type of recommendation")
priority: str = Field(..., description="Priority level")
what_to_update: str = Field(..., description="What needs to be updated")
where_to_update: str = Field(..., description="Where to make the update")
why_update_needed: str = Field(..., description="Why update is needed")
how_to_update: str = Field(..., description="How to perform the update")
affected_element_id: str = Field(..., description="ID of affected element")
affected_element_type: str = Field(..., description="Type of affected element")
confidence_score: float = Field(..., ge=0.0, le=1.0, description="Confidence score")
status: RecommendationStatus = Field(default=RecommendationStatus.PENDING)
⋮----
__all__ = [
</file>

<file path="agent/workflows/__init__.py">
__all__ = [
</file>

<file path="agent/workflows/baseline_map_creator.py">
VectorSearchRepository = None
⋮----
logger = logging.getLogger(__name__)
⋮----
@dataclass
class BaselineMapCreatorState
⋮----
repository: str
branch: str
⋮----
srs_content: Dict[str, str] = None
sdd_content: Dict[str, str] = None
code_files: List[Dict[str, Any]] = None
⋮----
requirements: List[RequirementModel] = None
design_elements: List[DesignElementModel] = None
code_components: List[CodeComponentModel] = None
⋮----
design_to_design_links: List[TraceabilityLinkModel] = None
design_to_code_links: List[TraceabilityLinkModel] = None
requirements_to_design_links: List[TraceabilityLinkModel] = None
⋮----
traceability_links: List[TraceabilityLinkModel] = None
⋮----
current_step: str = "initializing"
errors: List[str] = None
processing_stats: Dict[str, int] = None
⋮----
class BaselineMapCreatorWorkflow
⋮----
def _build_workflow(self) -> StateGraph
⋮----
workflow = StateGraph(BaselineMapCreatorState)
⋮----
async def execute(self, repository: str, branch: str = "main") -> BaselineMapCreatorState
⋮----
initial_state = BaselineMapCreatorState(
⋮----
existing_map = await self.baseline_map_repo.get_baseline_map(repository, branch)
⋮----
choice = input("Baseline map exists. Overwrite? (y/N): ").strip().lower()
⋮----
app = self.workflow.compile(checkpointer=self.memory)
config = {"configurable": {"thread_id": f"baseline_{repository.replace('/', '_')}_{branch}"}}
⋮----
final_state = await app.ainvoke(initial_state, config=config)
⋮----
async def _scan_repository(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
# Look for SDD files first (priority since they contain traceability matrices)
sdd_patterns = [
⋮----
# Look for SRS files (common patterns)
srs_patterns = [
⋮----
# Look for code files (common patterns)
code_patterns = ["*.py", "*.java", "*.js", "*.ts", "*.cpp", "*.h"]
⋮----
# Scan SDD first (contains traceability matrix)
⋮----
# Then scan SRS and code files
⋮----
error_msg = f"Error scanning repository: {str(e)}"
⋮----
async def _identify_design_elements(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
design_elements = []
elem_counter = 1
⋮----
# Use LLM to extract design elements
extracted_elements = await self._llm_extract_design_elements(content, file_path)
⋮----
design_element = DesignElementModel(
⋮----
# Generate embeddings for design elements immediately for better mapping accuracy
⋮----
error_msg = f"Error identifying design elements: {str(e)}"
⋮----
async def _design_to_design_mapping(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
design_to_design_links = []
link_counter = 1
⋮----
# Analyze relationships between design elements using embeddings + LLM
⋮----
links_data = await self._create_design_element_relationships_with_embeddings(state.design_elements)
⋮----
link = TraceabilityLinkModel(
⋮----
error_msg = f"Error creating design-to-design mappings: {str(e)}"
⋮----
async def _design_to_code_mapping(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
# Create code components from file paths (simplified approach)
code_components = []
comp_counter = 1
⋮----
file_path = file_info.get("path", "")
⋮----
# Create a code component for each file path
code_component = CodeComponentModel(
⋮----
name=Path(file_path).name  # Use full filename instead of stem
⋮----
# Generate embeddings for code components for better mapping accuracy
⋮----
# Create design-to-code links using embeddings for similarity matching
design_to_code_links = []
⋮----
links_data = await self._create_design_code_links_with_embeddings(state.design_elements, code_components)
⋮----
error_msg = f"Error creating design-to-code mappings: {str(e)}"
⋮----
async def _identify_requirements(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
requirements = []
req_counter = 1
⋮----
# Use LLM to extract requirements
extracted_reqs = await self._llm_extract_requirements(content, file_path)
⋮----
requirement = RequirementModel(
⋮----
# Generate embeddings for requirements for better mapping accuracy
⋮----
error_msg = f"Error identifying requirements: {str(e)}"
⋮----
async def _requirements_to_design_mapping(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
requirements_to_design_links = []
⋮----
# Extract traceability matrix from SDD and create requirement-design links using embeddings
req_to_design_links = await self._create_requirement_design_links_from_sdd_with_embeddings(
⋮----
# Combine all traceability links
⋮----
error_msg = f"Error creating requirements-to-design mappings: {str(e)}"
⋮----
async def _generate_embeddings(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
# Convert to dict format for embedding generation
baseline_map_data = {
⋮----
# Generate and store embeddings
success = await self.vector_search_repo.generate_and_store_embeddings(baseline_map_data)
⋮----
error_msg = f"Error generating embeddings: {str(e)}"
⋮----
async def _save_baseline_map(self, state: BaselineMapCreatorState) -> BaselineMapCreatorState
⋮----
# Create baseline map model
baseline_map = BaselineMapModel(
⋮----
# Save to database
success = await self.baseline_map_repo.save_baseline_map(baseline_map)
⋮----
error_msg = "Failed to save baseline map to database"
⋮----
error_msg = f"Error saving baseline map: {str(e)}"
⋮----
# Helper methods for GitHub API integration
async def _fetch_documentation_files(self, repository: str, patterns: List[str], branch: str) -> Dict[str, str]
⋮----
documentation_files = {}
⋮----
# Get repository
repo = self.github_client.get_repo(repository)
⋮----
# Search for documentation files
matching_files = await self._find_files_by_patterns(repo, patterns, branch)
⋮----
# Fetch content for each matching file
⋮----
content = await self._get_file_content(repo, file_path, branch)
⋮----
async def _fetch_code_files(self, repository: str, patterns: List[str], branch: str) -> List[Dict[str, Any]]
⋮----
code_files = []
⋮----
# Search for code files
⋮----
# Create file info for each matching file (without content for performance)
⋮----
"content": ""  # We'll only fetch content if needed for embedding generation
⋮----
async def _find_files_by_patterns(self, repo: Repository, patterns: List[str], branch: str) -> List[str]
⋮----
matching_files = []
⋮----
contents = repo.get_contents("", ref=branch)
⋮----
matching_files = await self._search_files_recursive(repo, contents, patterns, branch)
⋮----
full_path = f"{current_path}/{content.path}" if current_path else content.path
⋮----
skip_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'venv', 'env',
⋮----
# Recursively search directories
⋮----
# Check rate limit before each directory access
⋮----
sub_contents = repo.get_contents(content.path, ref=branch)
sub_matches = await self._search_files_recursive(
⋮----
await asyncio.sleep(1)  # Brief pause
⋮----
# Check if file matches any pattern
⋮----
def _matches_patterns(self, file_path: str, patterns: List[str]) -> bool
⋮----
# Handle glob patterns (*.py) and exact matches
⋮----
# Also check just the filename
filename = os.path.basename(file_path)
⋮----
async def _get_file_content(self, repo: Repository, file_path: str, branch: str) -> Optional[str]
⋮----
file_content = repo.get_contents(file_path, ref=branch)
⋮----
# Handle single file (not a list)
⋮----
file_content = file_content[0]
⋮----
# Decode content
⋮----
content = base64.b64decode(file_content.content).decode('utf-8')
⋮----
content = file_content.content
⋮----
async def _check_rate_limit(self)
⋮----
rate_limit = self.github_client.get_rate_limit()
core_remaining = rate_limit.core.remaining
⋮----
if core_remaining < 10:  # Less than 10 requests remaining
reset_time = rate_limit.core.reset.timestamp()
wait_time = max(0, reset_time - time.time())
⋮----
async def _llm_extract_requirements(self, content: str, file_path: str) -> List[Dict[str, Any]]
⋮----
# Placeholder - would implement LLM extraction
⋮----
async def _llm_extract_design_elements(self, content: str, file_path: str) -> List[Dict[str, Any]]
⋮----
async def _generate_design_element_embeddings(self, design_elements: List[DesignElementModel]) -> None
⋮----
# Create text representation for embedding
text = f"{element.name}: {element.description} (Type: {element.type})"
embedding = await self.embedding_client.generate_embedding(text)
# Store embedding (would be saved to vector database in full implementation)
⋮----
async def _generate_code_component_embeddings(self, code_components: List[CodeComponentModel]) -> None
⋮----
# Create text representation for embedding (using file path and name)
text = f"File: {component.path} ({component.name})"
⋮----
async def _generate_requirement_embeddings(self, requirements: List[RequirementModel]) -> None
⋮----
text = f"{requirement.title}: {requirement.description} (Type: {requirement.type}, Priority: {requirement.priority})"
⋮----
async def _create_design_element_relationships_with_embeddings(self, design_elements: List[DesignElementModel]) -> List[Dict[str, Any]]
⋮----
relationships = []
⋮----
# Calculate similarity between all pairs of design elements
⋮----
if i >= j:  # Avoid duplicates and self-references
⋮----
# Calculate similarity using embeddings (if available)
⋮----
similarity = await self._calculate_embedding_similarity(
⋮----
# Create relationship if similarity is above threshold
if similarity > 0.7:  # Threshold for relationship
⋮----
# Fallback to simple placeholder
relationships = [
⋮----
links = []
⋮----
# First try to parse explicit traceability matrix from SDD
explicit_links = await self._parse_traceability_matrix_from_sdd(sdd_content, requirements, design_elements)
⋮----
# Then use embedding similarity for additional mappings
similarity_links = await self._create_requirement_design_similarity_links(requirements, design_elements)
⋮----
links = [
⋮----
async def _create_design_code_links_with_embeddings(self, design_elements: List[DesignElementModel], code_components: List[CodeComponentModel]) -> List[Dict[str, Any]]
⋮----
# Use embedding similarity to match design elements with code files
⋮----
# Create link if similarity is above threshold
if similarity > 0.6:  # Lower threshold for design-to-code mapping
⋮----
async def _calculate_embedding_similarity(self, embedding1: List[float], embedding2: List[float]) -> float
⋮----
# Convert to numpy arrays
vec1 = np.array(embedding1)
vec2 = np.array(embedding2)
⋮----
# Calculate cosine similarity
dot_product = np.dot(vec1, vec2)
norm1 = np.linalg.norm(vec1)
norm2 = np.linalg.norm(vec2)
⋮----
similarity = dot_product / (norm1 * norm2)
⋮----
# Placeholder - would implement parsing of traceability tables/matrices in SDD
⋮----
__all__ = ["BaselineMapCreatorWorkflow", "BaselineMapCreatorState", "create_baseline_map_creator"]
</file>

<file path="agent/workflows/baseline_map_updater.py">
logger = logging.getLogger(__name__)
⋮----
@dataclass
class BaselineMapUpdaterState
⋮----
repository: str
branch: str
merged_pr_number: Optional[int] = None
⋮----
current_baseline_map: Optional[BaselineMapModel] = None
⋮----
pr_changes: List[Dict[str, Any]] = None
changed_files: Set[str] = None
commit_messages: List[str] = None
⋮----
impacted_requirements: List[RequirementModel] = None
impacted_design_elements: List[DesignElementModel] = None
impacted_code_components: List[CodeComponentModel] = None
⋮----
new_requirements: List[RequirementModel] = None
new_design_elements: List[DesignElementModel] = None
new_code_components: List[CodeComponentModel] = None
new_traceability_links: List[TraceabilityLinkModel] = None
⋮----
current_step: str = "initializing"
errors: List[str] = None
update_stats: Dict[str, int] = None
⋮----
class BaselineMapUpdaterWorkflow
⋮----
def _build_workflow(self) -> StateGraph
⋮----
workflow = StateGraph(BaselineMapUpdaterState)
⋮----
initial_state = BaselineMapUpdaterState(
⋮----
app = self.workflow.compile(checkpointer=self.memory)
config = {"configurable": {"thread_id": f"update_{repository.replace('/', '_')}_{branch}_{merged_pr_number}"}}
⋮----
final_state = await app.ainvoke(initial_state, config=config)
⋮----
async def _load_baseline_map(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
baseline_map = await self.baseline_map_repo.get_baseline_map(state.repository, state.branch)
⋮----
error_msg = f"No baseline map found for {state.repository}:{state.branch}"
⋮----
error_msg = f"Error loading baseline map: {str(e)}"
⋮----
async def _analyze_pr_changes(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
pr_data = await self._fetch_pr_data(state.repository, state.merged_pr_number)
⋮----
error_msg = f"Error analyzing PR changes: {str(e)}"
⋮----
async def _identify_impacts(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
impacted_requirements = []
impacted_design_elements = []
impacted_code_components = []
⋮----
filename = change.get("filename", "")
patch = change.get("patch", "")
⋮----
related_elements = await self.vector_search_repo.find_related_elements_by_code_change(
⋮----
req = RequirementModel(**req_data)
⋮----
de = DesignElementModel(**de_data)
⋮----
cc = CodeComponentModel(**cc_data)
⋮----
error_msg = f"Error identifying impacts: {str(e)}"
⋮----
async def _extract_new_elements(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
new_requirements = []
new_design_elements = []
new_code_components = []
⋮----
doc_files = [f for f in state.changed_files if f.endswith(('.md', '.txt', '.rst', '.doc', '.docx'))]
⋮----
change_data = next((c for c in state.pr_changes if c.get("filename") == file_path), None)
⋮----
added_content = self._extract_added_content(change_data.get("patch", ""))
⋮----
extracted_reqs = await self._llm_extract_requirements_from_content(added_content, file_path)
⋮----
extracted_elements = await self._llm_extract_design_elements_from_content(added_content, file_path)
⋮----
code_files = [f for f in state.changed_files if any(f.endswith(ext) for ext in ['.py', '.java', '.js', '.ts', '.cpp', '.h', '.cs'])]
⋮----
new_component = CodeComponentModel(
⋮----
extracted_components = await self._extract_components_from_code_addition(file_path, added_content)
⋮----
error_msg = f"Error extracting new elements: {str(e)}"
⋮----
async def _update_traceability_links(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
new_links = []
⋮----
all_design_elements = state.current_baseline_map.design_elements + state.new_design_elements
new_req_design_links = await self._create_requirement_design_links(
⋮----
all_code_components = state.current_baseline_map.code_components + state.new_code_components
new_design_code_links = await self._create_design_code_links(
⋮----
updated_links = await self._update_existing_links(state)
⋮----
error_msg = f"Error updating traceability links: {str(e)}"
⋮----
async def _update_embeddings(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
updated_baseline_map = {
⋮----
success = await self.vector_search_repo.generate_and_store_embeddings(updated_baseline_map)
⋮----
error_msg = f"Error updating embeddings: {str(e)}"
⋮----
async def _save_updated_map(self, state: BaselineMapUpdaterState) -> BaselineMapUpdaterState
⋮----
updated_baseline_map = BaselineMapModel(
⋮----
success = await self.baseline_map_repo.save_baseline_map(updated_baseline_map)
⋮----
error_msg = "Failed to save updated baseline map to database"
⋮----
error_msg = f"Error saving updated baseline map: {str(e)}"
⋮----
async def _fetch_pr_data(self, repository: str, pr_number: int) -> Optional[Dict[str, Any]]
⋮----
def _extract_added_content(self, patch: str) -> str
⋮----
added_lines = []
⋮----
def _is_significant_code_addition(self, content: str) -> bool
⋮----
lines = [line.strip() for line in content.split('\n') if line.strip()]
⋮----
async def _llm_extract_requirements_from_content(self, content: str, file_path: str) -> List[RequirementModel]
⋮----
async def _llm_extract_design_elements_from_content(self, content: str, file_path: str) -> List[DesignElementModel]
⋮----
async def _extract_components_from_code_addition(self, file_path: str, content: str) -> List[CodeComponentModel]
⋮----
async def _create_requirement_design_links(self, requirements: List[RequirementModel], design_elements: List[DesignElementModel]) -> List[TraceabilityLinkModel]
⋮----
async def _create_design_code_links(self, design_elements: List[DesignElementModel], code_components: List[CodeComponentModel]) -> List[TraceabilityLinkModel]
⋮----
async def _update_existing_links(self, state: BaselineMapUpdaterState) -> List[TraceabilityLinkModel]
⋮----
__all__ = ["BaselineMapUpdaterWorkflow", "BaselineMapUpdaterState", "create_baseline_map_updater"]
</file>

<file path="agent/workflows/document_update_recommendator.py">
logger = logging.getLogger(__name__)
⋮----
@dataclass
class DocumentUpdateRecommendatorState
⋮----
repository: str
pr_number: int
branch: str
⋮----
baseline_map: Optional[BaselineMapModel] = None
code_changes: List[Dict[str, Any]] = field(default_factory=list)
⋮----
impacted_elements: List[ImpactAnalysisResultModel] = field(default_factory=list)
⋮----
recommendations: List[DocumentationRecommendationModel] = field(default_factory=list)
⋮----
pr_summary: str = ""
errors: List[str] = field(default_factory=list)
processing_stats: Dict[str, int] = field(default_factory=dict)
⋮----
class DocumentUpdateRecommendatorWorkflow
⋮----
def _build_workflow(self) -> StateGraph
⋮----
workflow = StateGraph(DocumentUpdateRecommendatorState)
⋮----
async def execute(self, pr_url: str) -> DocumentUpdateRecommendatorState
⋮----
pr_info = await self._parse_pr_url(pr_url)
initial_state = DocumentUpdateRecommendatorState(
⋮----
app = self.workflow.compile(checkpointer=self.memory)
config = {"configurable": {"thread_id": f"pr_{pr_info['repository'].replace('/', '_')}_{pr_info['pr_number']}"}}
⋮----
final_state = await app.ainvoke(initial_state, config=config)
⋮----
async def _load_baseline_map(self, state: DocumentUpdateRecommendatorState) -> DocumentUpdateRecommendatorState
⋮----
baseline_map_data = await self.baseline_map_repo.get_baseline_map(state.repository, state.branch)
⋮----
error_msg = f"Error loading baseline map: {str(e)}"
⋮----
async def _analyze_code_changes(self, state: DocumentUpdateRecommendatorState) -> DocumentUpdateRecommendatorState
⋮----
pr_data = await self._fetch_pr_data(state.repository, state.pr_number)
⋮----
change_stats = {
⋮----
error_msg = f"Could not fetch PR data for {state.repository}#{state.pr_number}"
⋮----
error_msg = f"Error analyzing code changes: {str(e)}"
⋮----
async def _identify_impacted_elements(self, state: DocumentUpdateRecommendatorState) -> DocumentUpdateRecommendatorState
⋮----
impacted_elements = []
⋮----
file_path = change.get("filename", "")
⋮----
matching_components = [
⋮----
related_links = [
⋮----
severity = self._calculate_impact_severity(change)
⋮----
impact = ImpactAnalysisResultModel(
⋮----
error_msg = f"Error identifying impacted elements: {str(e)}"
⋮----
async def _analyze_semantic_impact(self, state: DocumentUpdateRecommendatorState) -> DocumentUpdateRecommendatorState
⋮----
semantic_impacts = []
⋮----
filename = change.get("filename", "")
patch = change.get("patch", "")
⋮----
related_elements = await self.vector_search_repo.find_related_elements_by_code_change(
⋮----
existing_ids = {(impact.element_type, impact.element_id) for impact in state.impacted_elements}
⋮----
semantic_count = len(semantic_impacts)
⋮----
error_msg = f"Error in semantic impact analysis: {str(e)}"
⋮----
async def _generate_recommendations(self, state: DocumentUpdateRecommendatorState) -> DocumentUpdateRecommendatorState
⋮----
recommendations = []
⋮----
grouped_impacts = {}
⋮----
element_type = impact.element_type
⋮----
recommendation_data = await self._llm_generate_recommendation(
⋮----
recommendation = DocumentationRecommendationModel(
⋮----
general_recommendations = await self._llm_generate_general_recommendations(state.code_changes)
⋮----
error_msg = f"Error generating recommendations: {str(e)}"
⋮----
async def _create_pr_summary(self, state: DocumentUpdateRecommendatorState) -> DocumentUpdateRecommendatorState
⋮----
summary_data = await self._llm_create_pr_summary(
⋮----
error_msg = f"Error creating PR summary: {str(e)}"
⋮----
async def _parse_pr_url(self, pr_url: str) -> Dict[str, Any]
⋮----
async def _fetch_pr_data(self, repository: str, pr_number: int) -> Optional[Dict[str, Any]]
⋮----
def _calculate_impact_severity(self, change: Dict[str, Any]) -> ImpactSeverity
⋮----
additions = change.get("additions", 0)
deletions = change.get("deletions", 0)
total_changes = additions + deletions
⋮----
async def _llm_generate_general_recommendations(self, code_changes: List[Dict[str, Any]]) -> List[DocumentationRecommendationModel]
⋮----
def create_document_update_recommendator(llm_client: Optional[DocurecoLLMClient] = None) -> DocumentUpdateRecommendatorWorkflow
⋮----
__all__ = ["DocumentUpdateRecommendatorWorkflow", "DocumentUpdateRecommendatorState", "create_document_update_recommendator"]
</file>

<file path="agent/__init__.py">

</file>

<file path="agent/baseline_map_creator.py">
logger = logging.getLogger(__name__)
⋮----
def main()
⋮----
repository = os.getenv("TARGET_REPOSITORY")
branch = os.getenv("TARGET_BRANCH", "main")
⋮----
async def create_baseline_map(repository: str, branch: str = "main")
⋮----
creator = create_baseline_map_creator()
⋮----
final_state = await creator.execute(repository, branch)
</file>

<file path="agent/baseline_map_updater.py">
logger = logging.getLogger(__name__)
⋮----
def main()
⋮----
repository = os.getenv("TARGET_REPOSITORY")
branch = os.getenv("TARGET_BRANCH", "main")
merged_pr_number = os.getenv("MERGED_PR_NUMBER")
⋮----
pr_number = None
⋮----
pr_number = int(merged_pr_number)
⋮----
async def update_baseline_map(repository: str, branch: str = "main", merged_pr_number: Optional[int] = None)
⋮----
updater = create_baseline_map_updater()
⋮----
final_state = await updater.execute(repository, branch, merged_pr_number)
⋮----
new_elements = (
</file>

<file path="agent/config.env.example">
# Docureco Agent Configuration

# LLM Configuration (Grok 3 Primary)
DOCURECO_LLM_PROVIDER=grok
DOCURECO_LLM_MODEL=grok-3-mini-reasoning-high
GROK_API_KEY=your_grok_api_key_here
GROK_BASE_URL=https://api.x.ai/v1

# LLM Parameters (Optional - will use defaults if not set)
DOCURECO_LLM_TEMPERATURE=0.1
DOCURECO_LLM_MAX_TOKENS=4000
DOCURECO_LLM_MAX_RETRIES=3
DOCURECO_LLM_TIMEOUT=120

# OpenAI Fallback Configuration (Optional)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# GitHub Integration (Set by GitHub Actions automatically)
GITHUB_TOKEN=your_github_token_here
GITHUB_EVENT_PATH=/github/workflow/event.json

# Database Configuration (Supabase for Traceability Map)
DATABASE_URL=postgresql://username:password@host:port/database
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your_supabase_anon_key_here
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key_here

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# Performance Configuration
MAX_CONCURRENT_OPERATIONS=5
REQUEST_TIMEOUT_SECONDS=120
MAX_FILE_SIZE_MB=10
</file>

<file path="agent/main.py">
logger = logging.getLogger(__name__)
⋮----
async def main()
⋮----
event_path = os.getenv("GITHUB_EVENT_PATH")
⋮----
event = json.load(f)
⋮----
pr_info = extract_pr_info(event)
⋮----
github_token = os.getenv("GITHUB_TOKEN")
⋮----
pr_details = await fetch_pr_details(pr_info, github_token)
changed_files = await fetch_changed_files(pr_info, github_token)
⋮----
llm_client = create_llm_client()
workflow = create_document_update_recommendator(llm_client)
⋮----
pr_url = f"https://github.com/{pr_info['repository']}/pull/{pr_info['pr_number']}"
final_state = await workflow.execute(pr_url)
⋮----
def extract_pr_info(event: Dict[str, Any]) -> Dict[str, Any]
⋮----
pr = event.get("pull_request", {})
repository_info = event.get("repository", {})
⋮----
async def fetch_pr_details(pr_info: Dict[str, Any], github_token: str) -> Dict[str, Any]
⋮----
repository = pr_info["repository"]
pr_number = pr_info["pr_number"]
⋮----
pr_url = f"https://api.github.com/repos/{repository}/pulls/{pr_number}"
⋮----
response = requests.get(
⋮----
pr_data = response.json()
⋮----
commits_url = f"https://api.github.com/repos/{repository}/pulls/{pr_number}/commits"
commits_response = requests.get(
⋮----
commits_data = commits_response.json()
⋮----
commit_messages = [commit["commit"]["message"] for commit in commits_data]
⋮----
async def fetch_changed_files(pr_info: Dict[str, Any], github_token: str) -> List[FileChangeModel]
⋮----
files_url = f"https://api.github.com/repos/{repository}/pulls/{pr_number}/files"
⋮----
files_data = response.json()
⋮----
changed_files = []
⋮----
file_change = FileChangeModel(
⋮----
async def load_baseline_map(repository: str) -> Dict[str, Any]
⋮----
comment_body = format_recommendations_comment(recommendations)
⋮----
comments_url = f"https://api.github.com/repos/{repository}/issues/{pr_number}/comments"
⋮----
response = requests.post(
⋮----
def format_recommendations_comment(recommendations: List[Any]) -> str
⋮----
comment_lines = [
⋮----
priority = getattr(rec, 'priority', rec.get('priority', 'Medium'))
target_document = getattr(rec, 'target_document', rec.get('target_document', 'Documentation'))
section = getattr(rec, 'section', rec.get('section', 'N/A'))
recommendation_type = getattr(rec, 'recommendation_type', rec.get('recommendation_type', 'UPDATE'))
what_to_update = getattr(rec, 'what_to_update', rec.get('what_to_update', 'Content needs updating'))
why_update_needed = getattr(rec, 'why_update_needed', rec.get('why_update_needed', 'Changes detected'))
how_to_update = getattr(rec, 'how_to_update', rec.get('how_to_update', 'Review and update content'))
⋮----
priority_emoji = {
⋮----
head_sha = pr_info["head_sha"]
⋮----
errors = getattr(workflow_state, 'errors', None) or workflow_state.get("errors", [])
recommendations = getattr(workflow_state, 'recommendations', None) or workflow_state.get("recommendations", [])
⋮----
status = "failure"
⋮----
high_priority = [
status = "action_required" if high_priority else "success"
⋮----
status = "success"
⋮----
github_status_map = {
⋮----
github_status = github_status_map.get(status, "failure")
⋮----
checks_url = f"https://api.github.com/repos/{repository}/check-runs"
⋮----
check_data = {
⋮----
def create_check_summary(workflow_state) -> str
⋮----
# Handle both dataclass and dict formats
⋮----
def run_async_main()
</file>

<file path="agent/README.md">
# Docureco Agent

Document Update Recommendator - AI Agent untuk analisis otomatis perubahan dokumentasi SRS dan SDD berdasarkan perubahan kode dalam GitHub Pull Requests.

## Overview

Docureco adalah agen inteligensi berbasis LLM (Large Language Model) yang dirancang untuk membantu pengembang menjaga sinkronisasi yang akurat antara kode sumber yang berevolusi dengan dokumentasi esensial, yaitu Software Requirements Specification (SRS) dan Software Design Document (SDD).

### Fitur Utama

- 🔍 **Analisis Perubahan Kode Otomatis**: Menganalisis perubahan kode dalam PR menggunakan framework 4W (What, Where, Why, How)
- 🗺️ **Traceability Mapping**: Memetakan hubungan antara kode, elemen desain, dan requirements
- 🎯 **Impact Assessment**: Menilai dampak perubahan kode terhadap dokumentasi SRS/SDD
- 📝 **Rekomendasi Cerdas**: Menghasilkan rekomendasi pembaruan dokumentasi yang spesifik dan actionable
- 🔗 **Integrasi GitHub**: Terintegrasi langsung dengan GitHub Actions dan PR workflow
- 🤖 **Powered by Grok 3**: Menggunakan Grok 3 Mini Reasoning (High) untuk analisis yang akurat

## Arsitektur

### Komponen Utama

1. **Document Update Recommendator**
   - PR Event Handler
   - Code Changes Analyzer
   - Impact Assessor
   - Recommendation Generator
   - Recommendation Poster

2. **Initial Baseline Map Creator**
   - Repo Artifact Fetcher
   - Mapping Engine
   - Map Storage Manager

3. **Baseline Map Updater**
   - Merged Changes Fetcher
   - Incremental Map Updater
   - Map Storage Manager

### Workflow

```mermaid
graph TD
    A[PR Opened/Updated] --> B[Scan PR Context]
    B --> C[Analyze Code Changes]
    C --> D[Assess Documentation Impact]
    D --> E[Generate Recommendations]
    E --> F[Post to PR & Update Status]
```

## Setup

### 1. Dependencies

Install dependencies menggunakan pip:

```bash
pip install -r requirements.txt
```

### 2. Environment Variables

Copy file `config.env.example` dan sesuaikan konfigurasi:

```bash
cp config.env.example .env
```

#### Required Variables

```env
# LLM Configuration
GROK_API_KEY=your_grok_api_key_here
GITHUB_TOKEN=your_github_token_here

# Database (Supabase)
DATABASE_URL=postgresql://username:password@host:port/database
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key_here
```

#### Optional Variables

```env
# LLM Provider (default: grok)
DOCURECO_LLM_PROVIDER=grok
DOCURECO_LLM_MODEL=grok-3-mini-reasoning-high

# OpenAI Fallback
OPENAI_API_KEY=your_openai_api_key_here

# Performance tuning
DOCURECO_LLM_TEMPERATURE=0.1
DOCURECO_LLM_MAX_TOKENS=4000
```

### 3. GitHub Actions Setup

Tambahkan secrets dan variables di repository settings:

#### Secrets
- `GROK_API_KEY`: API key untuk Grok 3
- `OPENAI_API_KEY`: API key OpenAI (fallback)
- `DATABASE_URL`: Connection string untuk database
- `SUPABASE_URL`: URL project Supabase
- `SUPABASE_SERVICE_ROLE_KEY`: Service role key Supabase

#### Variables (Optional)
- `DOCURECO_LLM_PROVIDER`: Provider LLM (default: grok)
- `DOCURECO_LLM_MODEL`: Model yang digunakan
- `LOG_LEVEL`: Level logging (default: INFO)

### 4. Repository Requirements

Untuk berfungsi optimal, repository harus memiliki:

- 📄 **Dokumen SRS** dalam format Markdown
- 📄 **Dokumen SDD** dalam format Markdown dengan traceability matrix
- 💻 **Source code** yang well-documented
- 📝 **Commit messages** yang deskriptif

## Usage

### Automatic Trigger

Docureco akan otomatis aktif ketika:
- Pull Request dibuka
- Pull Request di-update (push baru)
- Pull Request di-reopen

### Manual Trigger

Untuk membuat baseline traceability map:

1. Go to repository → Actions
2. Select "Docureco AI Agent" workflow
3. Click "Run workflow"
4. Choose branch dan jalankan

### Output

Docureco akan menghasilkan:

1. **PR Comments**: Rekomendasi pembaruan dokumentasi
2. **GitHub Checks**: Status analisis dan summary
3. **Logs**: Detail proses analisis untuk debugging

#### Contoh Output

```markdown
## 📋 Docureco Documentation Recommendations

Found **3** documentation update recommendations:

### 🟡 Recommendation 1: SDD
**Section:** DE_UserProfileModel
**Priority:** Moderate
**Action:** modify

Based on the addition of the UserProfile model, the SDD should be updated to include:
- Class diagram untuk UserProfile
- Description of data fields dan validation rules
- Integration dengan existing authentication system

---
```

## Configuration

### LLM Configuration

```python
# config/llm_config.py
class LLMConfig(BaseModel):
    provider: LLMProvider = Field(default=LLMProvider.GROK)
    model_name: str = Field(default="grok-3-mini-reasoning-high")
    temperature: float = Field(default=0.1)
    max_tokens: int = Field(default=4000)
```

### Task-Specific Settings

```python
# Berbeda temperature untuk berbagai tugas
code_analysis: temperature=0.1        # Konsisten untuk klasifikasi
traceability_mapping: temperature=0.2 # Sedikit lebih fleksibel
impact_assessment: temperature=0.15   # Balanced untuk penilaian
recommendation_generation: temperature=0.3  # Kreatif untuk text generation
```

## Development

### Project Structure

```
agent/
├── config/
│   ├── __init__.py
│   └── llm_config.py          # LLM configuration
├── llm/
│   ├── __init__.py
│   └── llm_client.py          # LLM client wrapper
├── models/
│   ├── __init__.py
│   └── docureco_models.py     # Pydantic models
├── workflows/
│   ├── __init__.py
│   └── docureco_workflow.py   # LangGraph workflow
├── main.py                    # Entry point
├── requirements.txt           # Dependencies
└── config.env.example        # Environment template
```

### Running Locally

```bash
# Set environment variables
export GROK_API_KEY="your_api_key"
export GITHUB_TOKEN="your_token"

# Run with mock PR data
python -m main
```

### Testing

```bash
# Run tests
pytest tests/

# Run with coverage
pytest tests/ --cov=agent
```

## API Reference

### Core Classes

#### DocurecoLLMClient

```python
from agent.llm import create_llm_client

client = create_llm_client()
response = await client.generate_response(
    prompt="Analyze this code change...",
    task_type="code_analysis",
    output_format="json"
)
```

#### DocurecoWorkflow

```python
from agent.workflows import create_docureco_workflow

workflow = create_docureco_workflow()
result = await workflow.execute({
    "pr_number": 123,
    "repository": "owner/repo",
    "changed_files": [...],
    "commit_messages": [...]
})
```

## Troubleshooting

### Common Issues

1. **LLM API Errors**
   ```
   Error: GROK_API_KEY not found
   Solution: Set GROK_API_KEY environment variable
   ```

2. **GitHub API Rate Limits**
   ```
   Solution: Ensure GITHUB_TOKEN has sufficient permissions
   ```

3. **Database Connection**
   ```
   Error: Connection to Supabase failed
   Solution: Verify DATABASE_URL dan SUPABASE_* variables
   ```

### Debugging

Enable debug logging:

```env
LOG_LEVEL=DEBUG
```

Check logs in GitHub Actions artifacts atau local output.

## Contributing

1. Fork repository
2. Create feature branch
3. Implement changes dengan tests
4. Update documentation
5. Submit Pull Request

## License

[MIT License](LICENSE)

## Support

Untuk pertanyaan atau issues:
- Create GitHub Issue
- Check existing documentation
- Review logs untuk error details

---

**Docureco Agent** - Keeping your documentation in sync with your code! 🚀
</file>

<file path="agent/requirements.txt">
# Core LangChain and LangGraph dependencies
langchain==0.3.7
langchain-core==0.3.17
langchain-openai==0.2.8
langgraph==0.2.34
langsmith==0.1.137

# HTTP clients and API interaction
httpx==0.27.2
requests==2.32.3
openai==1.54.3

# Database and storage
supabase==2.8.1
psycopg2-binary==2.9.9
sqlalchemy==2.0.35

# GitHub API and Git operations
PyGithub==2.4.0
GitPython==3.1.43

# Data processing and utilities
pydantic==2.9.2
python-dotenv==1.0.1
pyyaml==6.0.2
jinja2==3.1.4
numpy==1.26.4

# Logging and monitoring
structlog==24.4.0
rich==13.9.2

# Testing and development
pytest==8.3.3
pytest-asyncio==0.24.0
black==24.10.0
ruff==0.7.4
</file>

<file path="docs/SUPABASE_SETUP.md">

</file>

<file path=".gitignore">
# Python cache
__pycache__/
*.py[cod]
*.pyo

# Virtual environments
env/
.venv/

# Environment variables
.env

# Byte-compiled / optimized / DLL files
*.so

# Logs and databases
*.log
*.sqlite3

# GitHub Actions
.github/_temp/
.github/_work/

# VSCode
.vscode/

# Parsed PDF
parsed-pdf/
</file>

<file path="env.example">
# Docureco Agent Environment Configuration

# ==== LLM Configuration ====
# Primary LLM (Grok - recommended)
GROK_API_KEY=your_grok_api_key_here
GROK_BASE_URL=https://api.x.ai/v1

# Fallback LLM (OpenAI)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# LLM Settings
DOCURECO_LLM_PROVIDER=grok
DOCURECO_LLM_MODEL=grok-3-mini-reasoning-high
DOCURECO_LLM_TEMPERATURE=0.1
DOCURECO_LLM_MAX_TOKENS=4000
DOCURECO_LLM_MAX_RETRIES=3
DOCURECO_LLM_TIMEOUT=120

# ==== Database Configuration ====
# Supabase (Primary Database)
SUPABASE_URL=your_supabase_project_url
SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# ==== GitHub Integration ====
GITHUB_TOKEN=your_github_personal_access_token

# GitHub Actions (Auto-set)
# GITHUB_EVENT_PATH=/github/workflow/event.json
# GITHUB_REPOSITORY=owner/repo

# ==== Logging & Performance ====
LOG_LEVEL=INFO
MAX_CONCURRENT_OPERATIONS=5
REQUEST_TIMEOUT_SECONDS=120
MAX_FILE_SIZE_MB=10

# ==== Development Settings ====
# For testing without real PRs
TEST_MODE=false
TEST_PR_URL=https://github.com/owner/repo/pull/123
</file>

<file path="README.md">
# docureco

Docureco is an AI-powered GitHub Action that analyzes code changes and provides recommendations for updating the Software Requirements Specification (SRS) and Software Design Document (SDD).

## Tech Stack
- Python 3.x
- LangChain / LangGraph
- PostgreSQL
- Repomix
- GitHub Actions

## Setup
1. Copy `.env.example` to `.env` and fill in your credentials:
   ```bash
   cp .env.example .env
   ```
2. Install dependencies:
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```
3. Configure PostgreSQL database and ensure `DATABASE_URL` in your `.env` is correct.

## Local Usage
Run the agent locally to see recommendations based on the current working directory:
```bash
python -m docureco.main
```

## GitHub Action
A workflow is provided at `.github/workflows/agent.yml` that triggers on pull requests and posts recommendations as part of the CI process.
</file>

</files>
